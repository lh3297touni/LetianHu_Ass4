# helper_lib/utils.py
import os
import torch

def get_device():
    """
    è‡ªåŠ¨é€‰æ‹©å¯ç”¨è®¾å¤‡ï¼š
    - ä¼˜å…ˆä½¿ç”¨ CUDA
    - å†ä½¿ç”¨ Apple MPSï¼ˆé€‚ç”¨äº M1/M2ï¼‰
    - å¦åˆ™ä½¿ç”¨ CPU
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print("âœ… Using GPU (CUDA)")
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = torch.device("mps")
        print("âœ… Using Apple MPS")
    else:
        device = torch.device("cpu")
        print("âš™ï¸ Using CPU")
    return device


def save_model(model, path="model.pth"):
    """
    ä¿å­˜æ¨¡å‹å‚æ•°åˆ°æŒ‡å®šè·¯å¾„ã€‚
    """
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    torch.save(model.state_dict(), path)
    print(f"ğŸ’¾ Model saved to: {path}")


def load_model(model, path, device="cpu"):
    """
    ä»æ–‡ä»¶åŠ è½½æ¨¡å‹å‚æ•°ã€‚
    """
    model.load_state_dict(torch.load(path, map_location=device))
    model.to(device)
    print(f"ğŸ“‚ Model loaded from: {path}")
    return model


def print_progress(epoch, total_epochs, loss):
    """
    æ‰“å°è®­ç»ƒè¿›åº¦ä¿¡æ¯ã€‚
    """
    print(f"Epoch [{epoch+1}/{total_epochs}] - Loss: {loss:.4f}")
